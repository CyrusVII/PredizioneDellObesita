{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839968de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìò Obesity Prediction - Kaggle Playground S4E2 (REVISED)\n",
    "\n",
    "# === Imports ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from xgboost import XGBClassifier, plot_importance\n",
    "\n",
    "# === 1. Load data ===\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "display(train_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731eb92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. Data cleaning ===\n",
    "# Modifica chiave: uso .copy() per evitare SettingWithCopyWarning\n",
    "train_df = train_df.dropna().copy()\n",
    "test_df = test_df.dropna().copy()\n",
    "\n",
    "# Conversione esplicita yes/no -> True/False\n",
    "for df in [train_df, test_df]:\n",
    "    df.replace({'yes': True, 'no': False}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477373f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. Exploratory Data Analysis before encoding ===\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=train_df, x='NObeyesdad', order=train_df['NObeyesdad'].value_counts().index, palette='Set2')\n",
    "plt.title('Distribution of Obesity Risk Categories')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=train_df, x='NObeyesdad', y='Age', order=train_df['NObeyesdad'].value_counts().index, palette='Pastel1')\n",
    "plt.title('Age Distribution by Obesity Category')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=train_df, x='NObeyesdad', hue='Gender', order=train_df['NObeyesdad'].value_counts().index, palette='Set3')\n",
    "plt.title('Obesity Category Distribution by Gender')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=train_df, x='NObeyesdad', hue='family_history_with_overweight', order=train_df['NObeyesdad'].value_counts().index, palette='Set1')\n",
    "plt.title('Obesity Category by Family History')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(data=train_df, x='NObeyesdad', y='FCVC', order=train_df['NObeyesdad'].value_counts().index, palette='Set2')\n",
    "plt.title('Vegetable Consumption (FCVC) by Obesity Category')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. Encode target labels ===\n",
    "weight_map = {\n",
    "    'Normal_Weight': 1,\n",
    "    'Insufficient_Weight': 0,\n",
    "    'Overweight_Level_I': 2,\n",
    "    'Overweight_Level_II': 3,\n",
    "    'Obesity_Type_I': 4,\n",
    "    'Obesity_Type_II': 5,\n",
    "    'Obesity_Type_III': 6\n",
    "}\n",
    "train_df['NObeyesdad'] = train_df['NObeyesdad'].map(weight_map).astype(int)\n",
    "\n",
    "# === 5. Encoding colonne categoriche (versione migliorata) ===\n",
    "categorical_cols = train_df.select_dtypes(include='object').columns\n",
    "\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    if col in test_df.columns:\n",
    "        # Versione pi√π robusta con gestione 'unknown'\n",
    "        le = LabelEncoder()\n",
    "        train_df[col] = le.fit_transform(train_df[col].astype(str))\n",
    "        \n",
    "        # Processamento test set pi√π sicuro\n",
    "        test_df[col] = test_df[col].astype(str)\n",
    "        mask = ~test_df[col].isin(le.classes_)\n",
    "        if mask.any():\n",
    "            test_df.loc[mask, col] = 'unknown'\n",
    "            le.classes_ = np.append(le.classes_, 'unknown')\n",
    "        test_df[col] = le.transform(test_df[col])\n",
    "        label_encoders[col] = le\n",
    "\n",
    "# === 6. Train/Validation Split ===\n",
    "X = train_df.drop(columns=['id', 'NObeyesdad'])\n",
    "y = train_df['NObeyesdad']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# === 7. Feature Scaling ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Preparazione test set pi√π robusta\n",
    "X_test = test_df.drop(columns=['id'], errors='ignore')\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# === 8. Modeling with XGBoost (versione migliorata) ===\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    early_stopping_rounds=10  # Aggiunto early stopping\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    "    n_jobs=-1  # Aggiunto parallel processing\n",
    ")\n",
    "\n",
    "grid_search.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    eval_set=[(X_val_scaled, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "\n",
    "# === 9. Validation Evaluation (migliorato) ===\n",
    "y_pred = best_model.predict(X_val_scaled)\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# Aggiunta confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=weight_map.keys(),\n",
    "            yticklabels=weight_map.keys())\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Aggiunta feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_importance(best_model, max_num_features=20)\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === 10. Predictions on Test Set (versione pi√π robusta) ===\n",
    "predictions = best_model.predict(X_test_scaled)\n",
    "predictions = np.clip(predictions, 0, 6).astype(int)  # Clip esplicito\n",
    "\n",
    "inverse_map = {v: k for k, v in weight_map.items()}\n",
    "pred_labels = [inverse_map.get(p, 'Unknown') for p in predictions]\n",
    "\n",
    "output = test_df[['id']].copy()\n",
    "output['NObeyesdad'] = pred_labels  # Rinominato per consistenza con la competizione\n",
    "output.to_csv(\"submission_revised.csv\", index=False)\n",
    "\n",
    "print(\"\\nSample predictions:\")\n",
    "print(output.head(10))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
